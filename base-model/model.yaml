apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/disable-gpu: "true"
    opendatahub.io/template-display-name: OpenVINO Model Server
    opendatahub.io/template-name: ovms
    openshift.io/display-name: base-model
  labels:
    name: base-model
    opendatahub.io/dashboard: "true"
  name: base-model
  namespace: base-model
spec:
  builtInAdapter:
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
    runtimeManagementPort: 8888
    serverType: ovms
  containers:
  - args:
    - --port=8001
    - --rest_port=8888
    - --config_path=/models/model_config_list.json
    - --file_system_poll_wait_seconds=0
    - --grpc_bind_address=127.0.0.1
    - --rest_bind_address=127.0.0.1
    image: quay.io/modh/openvino_model_server@sha256:c2d063dc4085455aae87f0d94e63cb7d88ba772662e888cb28f46226a8ac4542
    name: ovms
    resources:
      limits:
        cpu: "2"
        memory: 8Gi
      requests:
        cpu: "1"
        memory: 4Gi
  grpcDataEndpoint: port:8001
  grpcEndpoint: port:8085
  multiModel: true
  protocolVersions:
  - grpc-v1
  replicas: 1
  supportedModelFormats:
  - autoSelect: true
    name: openvino_ir
    version: opset1
  - autoSelect: true
    name: onnx
    version: "1"
  - autoSelect: true
    name: tensorflow
    version: "2"
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: Base model
    serving.kserve.io/deploymentMode: ModelMesh
  labels:
    name: base
    opendatahub.io/dashboard: "true"
  name: base
  namespace: base-model
spec:
  predictor:
    model:
      modelFormat:
        name: onnx
        version: "1"
      runtime: base-model
      storage:
        key: aws-connection-models
        path: default/model.onnx